{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><p align=\"center\">导包</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2668\\2730816867.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstruct\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import struct\n",
    "import numpy\n",
    "import torch, torchvision, os\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><p align=\"center\">加载数据集</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path='.\\mnist', name='train'): \n",
    "    '''  \n",
    "    path:数据集的路径\n",
    "    name:值为train,代表读取训练集和验证集\n",
    "    '''\n",
    "    labels_path = os.path.join(path,'%s-labels.idx1-ubyte'% name)\n",
    "    images_path = os.path.join(path,'%s-images.idx3-ubyte'% name)\n",
    "    with open(labels_path, 'rb') as lb:\n",
    "        _, _ = struct.unpack('>II',lb.read(8))\n",
    "        labels = numpy.fromfile(lb,dtype=numpy.uint8)\n",
    "    with open(images_path, 'rb') as img:\n",
    "        _, num, rows, cols = struct.unpack('>IIII',img.read(16))\n",
    "        images = numpy.fromfile(img,dtype=numpy.uint8).reshape(num, rows, cols)\n",
    "    return images, labels\n",
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><p align=\"center\">数据预处理</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mnist_dataset(Dataset):\n",
    "    def __init__(self, mode='train', data_path='./'):\n",
    "        super().__init__()\n",
    "        assert mode in ['train', 'val', 'test']\n",
    "        if mode=='train':\n",
    "            self.images, self.labels = load_mnist(path=data_path, name='train')\n",
    "            self.images = self.images[:55000]\n",
    "            self.labels = self.labels[:55000]\n",
    "        elif mode=='val':\n",
    "            self.images, self.labels = load_mnist(path=data_path, name='train')\n",
    "            self.images = self.images[55000:]\n",
    "            self.labels = self.labels[55000:]\n",
    "        elif mode=='test':\n",
    "            self.images, self.labels = load_mnist(path=data_path, name='t10k')\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        image = torch.from_numpy(image).float().unsqueeze(0)/255.0\n",
    "        label_t = torch.zeros((10))\n",
    "        label_t[int(label)] = 1.0 # 标签使用onehot编码\n",
    "        label = torch.tensor(label)\n",
    "        return {'image':image, 'label':label, 'onehot_label':label_t}\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><p align=\"center\">网络模型</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(            \n",
    "            nn.Conv2d(in_channels=1, out_channels=20, kernel_size=5, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(in_channels=20, out_channels=50, kernel_size=5, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(in_channels=50, out_channels=500, kernel_size=4, stride=1)\n",
    "        )\n",
    "        self.classifier = nn.Linear(in_features=500, out_features=n_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        logits = self.classifier(x)\n",
    "        return logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><p align=\"center\">训练</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer():\n",
    "    # 设置随机数种子\n",
    "    setup_seed(20)\n",
    "    # 训练时的 batch size\n",
    "    b_size = 10\n",
    "    # 训练的总遍历数\n",
    "    epoches = 20\n",
    "    # 定义损失函数\n",
    "    loss_fn = torch.nn.CrossEntropyLoss() \n",
    "    # 设置学习率\n",
    "    learning_rate = 1e-4 \n",
    "    # 每隔多少batch打印一次\n",
    "    show_time = 500\n",
    "    # 装载模型\n",
    "    model = LeNet(n_classes=10)\n",
    "    # 优化器\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # 装载训练集\n",
    "    train_set = mnist_dataset(mode='train', data_path='./mnist')\n",
    "    train_dataloader = DataLoader(train_set, batch_size=b_size)\n",
    "    # 装载验证集，默认batch size为1\n",
    "    val_set = mnist_dataset(mode='val', data_path='./mnist')\n",
    "    val_dataloader = DataLoader(val_set, batch_size=1)\n",
    "    for epoch in range(epoches):\n",
    "        # 使用训练集训练\n",
    "        model.train()\n",
    "        for index, data in enumerate(train_dataloader):\n",
    "            image = data['image']\n",
    "            label = data['onehot_label']\n",
    "            # 清空梯度\n",
    "            optimizer.zero_grad() \n",
    "            # 预测并计算损失\n",
    "            pred = model(image)\n",
    "            loss = loss_fn(pred, label)\n",
    "            # 梯度回传并更新\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if index%show_time==0:\n",
    "                print(f'[epoch]  {epoch}, [batch]  {index}, [loss]  {loss.item()}')\n",
    "        # 保存模型\n",
    "        if not os.path.exists('./checkpoint'):\n",
    "            os.mkdir('./checkpoint')\n",
    "        torch.save(model.state_dict(), f'./checkpoint/lenet_model_epoch{epoch}.pth')\n",
    "        # 使用验证集测试\n",
    "        model.eval()\n",
    "        count = 0\n",
    "        for index, data in enumerate(val_dataloader):\n",
    "            image = data['image']\n",
    "            label = data['label']\n",
    "            # 预测\n",
    "            with torch.no_grad():\n",
    "                pred = torch.argmax(model(image), dim=1)\n",
    "            if pred[0]==label[0]:\n",
    "                count+=1\n",
    "        print(f'[epoch] {epoch}, val_acc {count/index}')\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><p align=\"center\">测试</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch]  0, [batch]  0, [loss]  2.306887149810791\n",
      "[epoch]  0, [batch]  500, [loss]  0.18896833062171936\n",
      "[epoch]  0, [batch]  1000, [loss]  0.43068045377731323\n",
      "[epoch]  0, [batch]  1500, [loss]  0.2544028162956238\n",
      "[epoch]  0, [batch]  2000, [loss]  0.12092254310846329\n",
      "[epoch]  0, [batch]  2500, [loss]  0.04947734251618385\n",
      "[epoch] 0, val_acc 0.9635927185437088\n",
      "[epoch]  1, [batch]  0, [loss]  0.05424345284700394\n",
      "[epoch]  1, [batch]  500, [loss]  0.04955226927995682\n",
      "[epoch]  1, [batch]  1000, [loss]  0.2960999310016632\n",
      "[epoch]  1, [batch]  1500, [loss]  0.07838545739650726\n",
      "[epoch]  1, [batch]  2000, [loss]  0.03898770362138748\n",
      "[epoch]  1, [batch]  2500, [loss]  0.006833062972873449\n",
      "[epoch] 1, val_acc 0.9733946789357871\n",
      "[epoch]  2, [batch]  0, [loss]  0.026589656248688698\n",
      "[epoch]  2, [batch]  500, [loss]  0.02717437781393528\n",
      "[epoch]  2, [batch]  1000, [loss]  0.1867002248764038\n",
      "[epoch]  2, [batch]  1500, [loss]  0.04716789722442627\n",
      "[epoch]  2, [batch]  2000, [loss]  0.018790798261761665\n",
      "[epoch]  2, [batch]  2500, [loss]  0.0026418138295412064\n",
      "[epoch] 2, val_acc 0.9803960792158432\n",
      "[epoch]  3, [batch]  0, [loss]  0.015841325744986534\n",
      "[epoch]  3, [batch]  500, [loss]  0.021328020840883255\n",
      "[epoch]  3, [batch]  1000, [loss]  0.12070180475711823\n",
      "[epoch]  3, [batch]  1500, [loss]  0.027567356824874878\n",
      "[epoch]  3, [batch]  2000, [loss]  0.013971880078315735\n",
      "[epoch]  3, [batch]  2500, [loss]  0.0016676324885338545\n",
      "[epoch] 3, val_acc 0.9831966393278656\n",
      "[epoch]  4, [batch]  0, [loss]  0.011907157488167286\n",
      "[epoch]  4, [batch]  500, [loss]  0.016896817833185196\n",
      "[epoch]  4, [batch]  1000, [loss]  0.10134390741586685\n",
      "[epoch]  4, [batch]  1500, [loss]  0.017772730439901352\n",
      "[epoch]  4, [batch]  2000, [loss]  0.013225680217146873\n",
      "[epoch]  4, [batch]  2500, [loss]  0.0013510722201317549\n",
      "[epoch] 4, val_acc 0.9853970794158832\n",
      "[epoch]  5, [batch]  0, [loss]  0.011224821209907532\n",
      "[epoch]  5, [batch]  500, [loss]  0.01165051944553852\n",
      "[epoch]  5, [batch]  1000, [loss]  0.09599872678518295\n",
      "[epoch]  5, [batch]  1500, [loss]  0.011604380793869495\n",
      "[epoch]  5, [batch]  2000, [loss]  0.014271135441958904\n",
      "[epoch]  5, [batch]  2500, [loss]  0.001091302721761167\n",
      "[epoch] 5, val_acc 0.986997399479896\n",
      "[epoch]  6, [batch]  0, [loss]  0.012195897288620472\n",
      "[epoch]  6, [batch]  500, [loss]  0.008341758511960506\n",
      "[epoch]  6, [batch]  1000, [loss]  0.09798597544431686\n",
      "[epoch]  6, [batch]  1500, [loss]  0.00805302057415247\n",
      "[epoch]  6, [batch]  2000, [loss]  0.016222387552261353\n",
      "[epoch]  6, [batch]  2500, [loss]  0.0008208622457459569\n",
      "[epoch] 6, val_acc 0.9877975595119024\n",
      "[epoch]  7, [batch]  0, [loss]  0.013698833994567394\n",
      "[epoch]  7, [batch]  500, [loss]  0.006431447807699442\n",
      "[epoch]  7, [batch]  1000, [loss]  0.10122969001531601\n",
      "[epoch]  7, [batch]  1500, [loss]  0.005846707616001368\n",
      "[epoch]  7, [batch]  2000, [loss]  0.017942847684025764\n",
      "[epoch]  7, [batch]  2500, [loss]  0.0006188364350236952\n",
      "[epoch] 7, val_acc 0.9875975195039007\n",
      "[epoch]  8, [batch]  0, [loss]  0.014454608783125877\n",
      "[epoch]  8, [batch]  500, [loss]  0.0052196127362549305\n",
      "[epoch]  8, [batch]  1000, [loss]  0.10327319800853729\n",
      "[epoch]  8, [batch]  1500, [loss]  0.004290308337658644\n",
      "[epoch]  8, [batch]  2000, [loss]  0.019276876002550125\n",
      "[epoch]  8, [batch]  2500, [loss]  0.0004806017386727035\n",
      "[epoch] 8, val_acc 0.9877975595119024\n",
      "[epoch]  9, [batch]  0, [loss]  0.01372484304010868\n",
      "[epoch]  9, [batch]  500, [loss]  0.004436415620148182\n",
      "[epoch]  9, [batch]  1000, [loss]  0.09582345187664032\n",
      "[epoch]  9, [batch]  1500, [loss]  0.003238147823140025\n",
      "[epoch]  9, [batch]  2000, [loss]  0.021678216755390167\n",
      "[epoch]  9, [batch]  2500, [loss]  0.00037320275441743433\n",
      "[epoch] 9, val_acc 0.9877975595119024\n",
      "[epoch]  10, [batch]  0, [loss]  0.010991803370416164\n",
      "[epoch]  10, [batch]  500, [loss]  0.003513424890115857\n",
      "[epoch]  10, [batch]  1000, [loss]  0.08789496123790741\n",
      "[epoch]  10, [batch]  1500, [loss]  0.002453648019582033\n",
      "[epoch]  10, [batch]  2000, [loss]  0.024899879470467567\n",
      "[epoch]  10, [batch]  2500, [loss]  0.00026394391898065805\n",
      "[epoch] 10, val_acc 0.9875975195039007\n",
      "[epoch]  11, [batch]  0, [loss]  0.009321058169007301\n",
      "[epoch]  11, [batch]  500, [loss]  0.0032016970217227936\n",
      "[epoch]  11, [batch]  1000, [loss]  0.07515156269073486\n",
      "[epoch]  11, [batch]  1500, [loss]  0.001969260396435857\n",
      "[epoch]  11, [batch]  2000, [loss]  0.027066901326179504\n",
      "[epoch]  11, [batch]  2500, [loss]  0.00019323473679833114\n",
      "[epoch] 11, val_acc 0.9887977595519104\n",
      "[epoch]  12, [batch]  0, [loss]  0.009512731805443764\n",
      "[epoch]  12, [batch]  500, [loss]  0.0029105779249221087\n",
      "[epoch]  12, [batch]  1000, [loss]  0.061502307653427124\n",
      "[epoch]  12, [batch]  1500, [loss]  0.0015619299374520779\n",
      "[epoch]  12, [batch]  2000, [loss]  0.029984408989548683\n",
      "[epoch]  12, [batch]  2500, [loss]  0.00014837893832009286\n",
      "[epoch] 12, val_acc 0.9891978395679136\n",
      "[epoch]  13, [batch]  0, [loss]  0.008674127981066704\n",
      "[epoch]  13, [batch]  500, [loss]  0.0024634008295834064\n",
      "[epoch]  13, [batch]  1000, [loss]  0.05010420083999634\n",
      "[epoch]  13, [batch]  1500, [loss]  0.0012664448004215956\n",
      "[epoch]  13, [batch]  2000, [loss]  0.031404245644807816\n",
      "[epoch]  13, [batch]  2500, [loss]  0.0001095445841201581\n",
      "[epoch] 13, val_acc 0.9895979195839167\n",
      "[epoch]  14, [batch]  0, [loss]  0.007729052100330591\n",
      "[epoch]  14, [batch]  500, [loss]  0.002318317536264658\n",
      "[epoch]  14, [batch]  1000, [loss]  0.04286067187786102\n",
      "[epoch]  14, [batch]  1500, [loss]  0.001008631894364953\n",
      "[epoch]  14, [batch]  2000, [loss]  0.03129647299647331\n",
      "[epoch]  14, [batch]  2500, [loss]  8.041704131755978e-05\n",
      "[epoch] 14, val_acc 0.98999799959992\n",
      "[epoch]  15, [batch]  0, [loss]  0.006502189673483372\n",
      "[epoch]  15, [batch]  500, [loss]  0.0021490484941750765\n",
      "[epoch]  15, [batch]  1000, [loss]  0.0341128408908844\n",
      "[epoch]  15, [batch]  1500, [loss]  0.0008055103244259953\n",
      "[epoch]  15, [batch]  2000, [loss]  0.03362388163805008\n",
      "[epoch]  15, [batch]  2500, [loss]  6.468495848821476e-05\n",
      "[epoch] 15, val_acc 0.9901980396079216\n",
      "[epoch]  16, [batch]  0, [loss]  0.005113071296364069\n",
      "[epoch]  16, [batch]  500, [loss]  0.0021846971940249205\n",
      "[epoch]  16, [batch]  1000, [loss]  0.028001483529806137\n",
      "[epoch]  16, [batch]  1500, [loss]  0.0006360962288454175\n",
      "[epoch]  16, [batch]  2000, [loss]  0.03459080308675766\n",
      "[epoch]  16, [batch]  2500, [loss]  5.450024764286354e-05\n",
      "[epoch] 16, val_acc 0.9901980396079216\n",
      "[epoch]  17, [batch]  0, [loss]  0.0038627355825155973\n",
      "[epoch]  17, [batch]  500, [loss]  0.0023006219416856766\n",
      "[epoch]  17, [batch]  1000, [loss]  0.01668252982199192\n",
      "[epoch]  17, [batch]  1500, [loss]  0.0004995843628421426\n",
      "[epoch]  17, [batch]  2000, [loss]  0.03405027091503143\n",
      "[epoch]  17, [batch]  2500, [loss]  4.2891108023468405e-05\n",
      "[epoch] 17, val_acc 0.9903980796159232\n",
      "[epoch]  18, [batch]  0, [loss]  0.0025987119879573584\n",
      "[epoch]  18, [batch]  500, [loss]  0.0023106432054191828\n",
      "[epoch]  18, [batch]  1000, [loss]  0.014099744148552418\n",
      "[epoch]  18, [batch]  1500, [loss]  0.00038639435661025345\n",
      "[epoch]  18, [batch]  2000, [loss]  0.033811844885349274\n",
      "[epoch]  18, [batch]  2500, [loss]  3.648084748419933e-05\n",
      "[epoch] 18, val_acc 0.9903980796159232\n",
      "[epoch]  19, [batch]  0, [loss]  0.0017754066502675414\n",
      "[epoch]  19, [batch]  500, [loss]  0.0025321566499769688\n",
      "[epoch]  19, [batch]  1000, [loss]  0.011323124170303345\n",
      "[epoch]  19, [batch]  1500, [loss]  0.0002945067244581878\n",
      "[epoch]  19, [batch]  2000, [loss]  0.03287839516997337\n",
      "[epoch]  19, [batch]  2500, [loss]  3.0803152185399085e-05\n",
      "[epoch] 19, val_acc 0.9907981596319264\n",
      "[epoch] 5, test_acc 0.9903990399039904\n"
     ]
    }
   ],
   "source": [
    "# 装载模型\n",
    "model = trainer()\n",
    "# 装载验证集，默认batch size为1\n",
    "test_set = mnist_dataset(mode='test', data_path='./mnist')\n",
    "test_dataloader = DataLoader(test_set, batch_size=1)\n",
    "model.eval()\n",
    "count = 0\n",
    "for index, data in enumerate(test_dataloader):\n",
    "    image = data['image']\n",
    "    label = data['label']\n",
    "    with torch.no_grad():\n",
    "        pred = torch.argmax(model(image), dim=1)\n",
    "    if pred[0]==label[0]:\n",
    "        count+=1\n",
    "# 指定checkpoint\n",
    "use_epoch = 5\n",
    "print(f'[epoch] {use_epoch}, test_acc {count/index}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><p align=\"center\">绘制特征图</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Structure: \n",
      "Sequential(\n",
      "  (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (3): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (4): ReLU()\n",
      "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (6): Conv2d(50, 500, kernel_size=(4, 4), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 装载模型\n",
    "model = LeNet(n_classes=10)\n",
    "# 指定checkpoint\n",
    "use_epoch = 5\n",
    "# 加载模型\n",
    "model.load_state_dict(torch.load(f'./checkpoint/lenet_model_epoch{use_epoch}.pth'))\n",
    "# 装载验证集，默认batch size为1\n",
    "test_set = mnist_dataset(mode='test', data_path='./mnist')\n",
    "index_image = 10\n",
    "image = test_set.__getitem__(index_image)['image'].unsqueeze(0)\n",
    "feature_extractor = model.feature_extractor\n",
    "print('Network Structure: ')\n",
    "print(feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./feature_map'):\n",
    "    os.mkdir('./feature_map')\n",
    "for i in range(len(feature_extractor)):   \n",
    "    # 提取第i层的输出（包含激活函数和池化层）\n",
    "    feature_i = feature_extractor[:i+1](image) \n",
    "    # 转换成grid形式\n",
    "    feature_grid = torchvision.utils.make_grid(feature_i[0].unsqueeze(1),10,normalize=True,padding=0) \n",
    "    # 保存图片\n",
    "    torchvision.utils.save_image(feature_grid,f'./feature_map/{index_image}_{i}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><p align=\"center\">手写图像测试</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 装载模型\n",
    "model = LeNet(n_classes=10)\n",
    "# 指定checkpoint\n",
    "use_epoch = 5\n",
    "# 加载模型\n",
    "model.load_state_dict(torch.load(f'./checkpoint/lenet_model_epoch{use_epoch}.pth'))\n",
    "image = cv2.imread('./mytest/test1.png', cv2.COLOR_BGR2RGB)\n",
    "img = torch.from_numpy(image).float().unsqueeze(0)/255\n",
    "img = img.unsqueeze(dim = 0)\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prbi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
